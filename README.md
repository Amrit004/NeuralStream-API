ğŸš€ NeuralStream
Real-Time ML Feature Store & Model Serving Platform

NeuralStream is a production-style machine learning infrastructure platform built with FastAPI. It provides:

âš¡ Real-time feature store

ğŸ“¦ Model registry with versioning & lineage

ğŸ¯ A/B model traffic splitting

ğŸ”® Real-time and batch prediction endpoints

ğŸ“Š Drift monitoring & prediction statistics

ğŸ— Architecture Overview

NeuralStream consists of three core components:

1ï¸âƒ£ Feature Store

Online feature retrieval

Feature views with TTL

Batch and single-entity reads

Redis-backed storage

2ï¸âƒ£ Model Registry

Model version tracking

Promotion system (staging â†’ production)

A/B traffic splitting

Lineage tracking

3ï¸âƒ£ Serving Engine

Automatic feature retrieval

Real-time predictions

Batch inference

Drift detection

Latency tracking

ğŸ“ Project Structure
neuralstream/
â”‚
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ feature_store/
â”‚   â””â”€â”€ store.py
â”‚
â”œâ”€â”€ model_registry/
â”‚   â””â”€â”€ registry.py
â”‚
â”œâ”€â”€ serving/
â”‚   â””â”€â”€ engine.py
âš™ï¸ Installation
1ï¸âƒ£ Clone the repository
git clone https://github.com/YOUR_USERNAME/neuralstream.git
cd neuralstream
2ï¸âƒ£ Create virtual environment
python -m venv venv

Activate it:

Windows (Git Bash):

source venv/Scripts/activate

PowerShell:

venv\Scripts\activate
3ï¸âƒ£ Install dependencies
pip install -r requirements.txt
4ï¸âƒ£ Start Redis

If using Docker:

docker run -p 6379:6379 redis
5ï¸âƒ£ Run the API
uvicorn main:app --reload

Server will run at:

http://127.0.0.1:8000

Swagger docs:

http://127.0.0.1:8000/docs
ğŸ“Œ Core API Endpoints
Health Check
GET /health
Feature Store

Write Features:

POST /features/write

Get Features:

GET /features/{feature_view}/{entity_id}

Batch Read:

POST /features/batch

List Feature Views:

GET /features/views
Model Registry

List Models:

GET /models

Model Lineage:

GET /models/{name}/lineage

Promote Model:

POST /models/{name}/promote/{version}

Configure A/B Split:

POST /models/{name}/ab-split
Serving

Real-time Prediction:

POST /predict/{model_name}

Batch Prediction:

POST /predict/{model_name}/batch
Monitoring

Drift Report:

GET /monitoring/drift

Prediction Stats:

GET /monitoring/stats
ğŸ§  Example Prediction Request
POST /predict/churn_model
{
  "entity_id": "user_123",
  "feature_view": "user_features",
  "return_features": true
}

Response:

{
  "model": "churn_model",
  "version": "v1",
  "entity_id": "user_123",
  "prediction": 1,
  "probability": 0.87,
  "latency_ms": 12.3,
  "features": {...},
  "timestamp": "2026-02-21T10:15:00"
}
ğŸ›  Technologies Used

FastAPI

Uvicorn

Redis (async)

Pydantic

NumPy

ğŸ¯ Key Features

Production-style ML infrastructure design

Clean separation of feature store, registry, and serving

Real-time inference

A/B model routing

Drift monitoring

Extensible architecture

ğŸš€ Future Improvements

Persistent model storage

Authentication & API keys

Metrics dashboard (Prometheus/Grafana)

Kubernetes deployment

CI/CD integration

ğŸ‘¤ Author

Amritpal Singh Kaur
